{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What is Text Classification?\n",
        "\n",
        "Text classification is a common NLP task used to solve business problems in various fields. The goal of text classification is to categorize or predict a class of unseen text documents, often with the help of supervised machine learning.\n",
        "\n",
        "Similar to a classification algorithm that has been trained on a tabular dataset to predict a class, text classification also uses supervised machine learning. The fact that text is involved in text classification is the main distinction between the two.\n",
        "\n",
        "You can also perform text classification without using supervised machine learning. Instead of algorithms, a manual rule-based system can be designed to perform the task of text classification. We’ll compare and review the pros and cons of rule-based and machine-learning-based text classification systems in the next section."
      ],
      "metadata": {
        "id": "x-4j6TSuQflc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Classification Use-Cases and Applications\n",
        "\n",
        "Spam classification\n",
        "\n",
        "There are many practical use cases for text classification across many industries. For example, a spam filter is a common application that uses text classification to sort emails into spam and non-spam categories."
      ],
      "metadata": {
        "id": "GnPKFeOLQi4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifying news articles and blogs\n",
        "\n",
        "Another use case is to automatically assign text documents into predetermined categories. A supervised machine learning model is trained on labeled data, which includes both the raw text and the target. Once a model is trained, it is then used in production to obtain a category (label) on the new and unseen data (articles/blogs written in the future)."
      ],
      "metadata": {
        "id": "g8WHkycCQmKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorize customer support requests\n",
        "\n",
        "A company might use text classification to automatically categorize customer support requests by topic or to prioritize and route requests to the appropriate department."
      ],
      "metadata": {
        "id": "2iU9K1W-QoWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hate speech detection\n",
        "\n",
        "With over 1.7 billion daily active users, Facebook inevitably has content created on the site that is against the rules. Hate speech is included in this undesirable content.\n",
        "\n",
        "Facebook tackles this issue by requesting a manual review of postings that an AI text classifier has identified as hate speech. Postings that were flagged by AI are examined in the same manner as posts that users have reported. In fact, in just the first three months of 2020, the platform removed 9.6 million items of content that had been classified as hate speech.\n",
        "\n"
      ],
      "metadata": {
        "id": "yn7hsxMVQqsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types of Text Classification Systems\n",
        "\n",
        "There are mainly two types of text classification systems; rule-based and machine learning-based text classification.\n",
        "\n",
        "Rule-based text classification\n",
        "\n",
        "Rule-based techniques use a set of manually constructed language rules to categorize text into categories or groups. These rules tell the system to classify text into a particular category based on the content of a text by using semantically relevant textual elements. An antecedent or pattern and a projected category make up each rule.\n",
        "\n",
        "For example, imagine you have tons of new articles, and your goal is to assign them to relevant categories such as Sports, Politics, Economy, etc.\n",
        "\n",
        "With a rule-based classification system, you will do a human review of a couple of documents to come up with linguistic rules like this one:\n",
        "\n",
        "If the document contains words such as money, dollar, GDP, or inflation, it belongs to the Politics group (class).\n",
        "Rule-based systems can be refined over time and are understandable to humans. However, there are certain drawbacks to this strategy.\n",
        "\n",
        "These systems, to begin with, demand in-depth expertise in the field. They take a lot of time since creating rules for a complicated system can be difficult and frequently necessitates extensive study and testing.\n",
        "\n",
        "Given that adding new rules can alter the outcomes of the pre-existing rules, rule-based systems are also challenging to maintain and do not scale effectively.\n",
        "\n",
        "Machine learning-based text classification\n",
        "\n",
        "Machine learning-based text classification is a supervised machine learning problem. It learns the mapping of input data (raw text) with the labels (also known as target variables). This is similar to non-text classification problems where we train a supervised classification algorithm on a tabular dataset to predict a class, with the exception that in text classification, the input data is raw text instead of numeric features.\n",
        "\n",
        "Like any other supervised machine learning, text classification machine learning has two phases; training and prediction."
      ],
      "metadata": {
        "id": "U3kJqks_Qtk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training phase\n",
        "\n",
        "A supervised machine learning algorithm is trained on the input-labeled dataset during the training phase. At the end of this process, we get a trained model that we can use to obtain predictions (labels) on new and unseen data.\n",
        "\n",
        "Prediction phase\n",
        "\n",
        "Once a machine learning model is trained, it can be used to predict labels on new and unseen data. This is usually done by deploying the best model from an earlier phase as an API on the server.\n",
        "\n",
        "Text Preprocessing Pipeline\n",
        "\n",
        "Preprocessing text data is an important step in any natural language processing task. It helps in cleaning and preparing the text data for further processing or analysis.\n",
        "\n",
        "A text preprocessing pipeline is a series of processing steps that are applied to raw text data in order to prepare it for use in natural language processing tasks.\n",
        "\n",
        "The steps in a text preprocessing pipeline can vary, but they typically include tasks such as tokenization, stop word removal, stemming, and lemmatization. These steps help reduce the size of the text data and also improve the accuracy of NLP tasks such as text classification and information extraction.\n",
        "\n",
        "Text data is difficult to process because it is unstructured and often contains a lot of noise. This noise can be in the form of misspellings, grammatical errors, and non-standard formatting. A text preprocessing pipeline aims to clean up this noise so that the text data can be more easily analyzed."
      ],
      "metadata": {
        "id": "VK2EkFpVQ0ZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction\n",
        "\n",
        "The two most common methods for extracting feature from text or in other words converting text data (strings) into numeric features so machine learning model can be trained are: Bag of Words (a.k.a CountVectorizer) and Tf-IDF.\n",
        "\n",
        "Bag of Words\n",
        "\n",
        "A bag of words (BoW) model is a simple way of representing text data as numeric features. It involves creating a vocabulary of known words in the corpus and then creating a vector for each document that contains counts of how often each word appears."
      ],
      "metadata": {
        "id": "k1L7yZPTQ3kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF\n",
        "\n",
        "TF-IDF stands for term frequency-inverse document frequency, and it is another way of representing text as numeric features. There are some shortcomings of the Bag of Words (BoW) model that Tf-IDF overcomes. We won’t go into detail about that in this article, but if you would like to explore this concept further, check out our Introduction to Natural Language Processing in Python course.\n",
        "\n",
        "The TF-IDF model is different from the bag of words model in that it takes into account the frequency of the words in the document, as well as the inverse document frequency. This means that the TF-IDF model is more likely to identify the important words in a document than the bag of words model.\n",
        "\n",
        "End-to-End Text Classification In Python Example\n",
        "\n",
        "Importing Dataset\n",
        "\n",
        "First, start by importing the dataset directly from this GitHub link. The SMS Spam Collection is a dataset containing 5,574 SMS messages in English along with the label Spam or Ham (not spam). Our goal is to train a machine learning model that will learn from the text of SMS and the label and be able to predict the class of SMS messages."
      ],
      "metadata": {
        "id": "MuHAhvR-Q50J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv', encoding='latin-1')\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "yKVIVTuDQ9Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop unnecessary columns and rename cols\n",
        "\n",
        "data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)\n",
        "\n",
        "data.columns = ['label', 'text']\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "zSH79TtWQ_O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values\n",
        "\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "SHUya0wQRAm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check data shape\n",
        "\n",
        "data.shape\n"
      ],
      "metadata": {
        "id": "DifaUjaMRDI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check target balance\n",
        "\n",
        "data['label'].value_counts(normalize = True).plot.bar()"
      ],
      "metadata": {
        "id": "G8g1U8o0RJmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text preprocessing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# download nltk\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('all')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create a list text\n",
        "\n",
        "text = list(data['text'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# preprocessing loop\n",
        "\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "corpus = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(text)):\n",
        "\n",
        "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
        "\n",
        "    r = r.lower()\n",
        "\n",
        "    r = r.split()\n",
        "\n",
        "    r = [word for word in r if word not in stopwords.words('english')]\n",
        "\n",
        "    r = [lemmatizer.lemmatize(word) for word in r]\n",
        "\n",
        "    r = ' '.join(r)\n",
        "\n",
        "    corpus.append(r)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#assign corpus to data['text']\n",
        "\n",
        "data['text'] = corpus\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "NVwW35iXRLIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Feature and Label sets\n",
        "\n",
        "X = data['text']\n",
        "\n",
        "y = data['label']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# train test split (66% train - 33% test)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Training Data :', X_train.shape)\n",
        "\n",
        "print('Testing Data : ', X_test.shape)"
      ],
      "metadata": {
        "id": "3lAy_BRFRM-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Bag of Words model\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "\n",
        "X_train_cv = cv.fit_transform(X_train)\n",
        "\n",
        "X_train_cv.shape"
      ],
      "metadata": {
        "id": "ZF9acy5JRPNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Logistic Regression model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "lr.fit(X_train_cv, y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# transform X_test using CV\n",
        "\n",
        "X_test_cv = cv.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# generate predictions\n",
        "\n",
        "predictions = lr.predict(X_test_cv)\n",
        "\n",
        "predictions"
      ],
      "metadata": {
        "id": "TnJi5jNwRWau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> array(['ham', 'spam', 'ham', ..., 'ham', 'ham', 'spam'], dtype=object)"
      ],
      "metadata": {
        "id": "yYsfNPQVRY0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "df = pd.DataFrame(metrics.confusion_matrix(y_test,predictions), index=['ham','spam'], columns=['ham','spam'])\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "HcuJjqjQRbEa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}