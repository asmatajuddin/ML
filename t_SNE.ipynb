{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " t-SNE"
      ],
      "metadata": {
        "id": "I0EkpC-pU30V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will delve into the workings of t-SNE, a powerful technique for dimensionality reduction and data visualization. We will compare it with another popular technique, PCA, and demonstrate how to perform both t-SNE and PCA using scikit-learn and plotly express on synthetic and real-world datasets."
      ],
      "metadata": {
        "id": "T-RwnnadU4ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is t-SNE ?\n",
        "\n",
        "t-SNE (t-distributed Stochastic Neighbor Embedding) is an unsupervised non-linear dimensionality reduction technique for data exploration and visualizing high-dimensional data. Non-linear dimensionality reduction means that the algorithm allows us to separate data that cannot be separated by a straight line."
      ],
      "metadata": {
        "id": "sNbLGllUU66m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE gives you a feel and intuition on how data is arranged in higher dimensions. It is often used to visualize complex datasets into two and three dimensions, allowing us to understand more about underlying patterns and relationships in the data.\n",
        "\n",
        "Take our Dimensionality Reduction in Python course to learn about exploring high-dimensional data, feature selection, and feature extraction.\n",
        "\n",
        "t-SNE vs PCA\n",
        "\n",
        "Both t-SNE and PCA are dimensional reduction techniques that have different mechanisms and work best with different types of data.\n",
        "\n",
        "PCA (Principal Component Analysis) is a linear technique that works best with data that has a linear structure. It seeks to identify the underlying principal components in the data by projecting onto lower dimensions, minimizing variance, and preserving large pairwise distances. Read our Principal Component Analysis (PCA) tutorial to understand the inner working of the algorithms with R examples.\n",
        "\n",
        "But, t-SNE is a nonlinear technique that focuses on preserving the pairwise similarities between data points in a lower-dimensional space. t-SNE is concerned with preserving small pairwise distances whereas, PCA focuses on maintaining large pairwise distances to maximize variance.\n",
        "\n",
        "In summary, PCA preserves the variance in the data, whereas t-SNE preserves the relationships between data points in a lower-dimensional space, making it quite a good algorithm for visualizing complex high-dimensional data.\n",
        "\n",
        "How t-SNE works\n",
        "\n",
        "The t-SNE algorithm finds the similarity measure between pairs of instances in higher and lower dimensional space. After that, it tries to optimize two similarity measures. It does all of that in three steps.\n",
        "\n",
        "t-SNE models a point being selected as a neighbor of another point in both higher and lower dimensions. It starts by calculating a pairwise similarity between all data points in the high-dimensional space using a Gaussian kernel. The points that are far apart have a lower probability of being picked than the points that are close together.\n",
        "Then, the algorithm tries to map higher dimensional data points onto lower dimensional space while preserving the pairwise similarities.\n",
        "It is achieved by minimizing the divergence between the probability distribution of the original high-dimensional and lower-dimensional. The algorithm uses gradient descent to minimize the divergence. The lower-dimensional embedding is optimized to a stable state.\n",
        "The optimization process allows the creation of clusters and sub-clusters of similar data points in the lower-dimensional space that are visualized to understand the structure and relationship in the higher-dimensional data.\n",
        "\n",
        "t-SNE Python Example\n",
        "\n",
        "In the Python example, we will generate classification data, perform PCA and t-SNE, and visualize the results. For performing dimensionality reduction, we will use Scikit-Learn, and for visualization, we will use Plotly Express.\n",
        "\n",
        "Generating Classification Dataset\n",
        "\n",
        "We will use Scikit-Learnâ€™s make_classification function to generate synthetic data with 6 features, 1500 samples, and 3 classes.\n",
        "\n",
        "After that, we will 3D plot the first three features of the data using the Plotly Express scatter_3d function."
      ],
      "metadata": {
        "id": "ygAZ6nTtU9su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_features=6,\n",
        "    n_classes=3,\n",
        "    n_samples=1500,\n",
        "    n_informative=2,\n",
        "    random_state=5,\n",
        "    n_clusters_per_class=1,\n",
        ")\n",
        "\n",
        "\n",
        "fig = px.scatter_3d(x=X[:, 0], y=X[:, 1], z=X[:, 2], color=y, opacity=0.8)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "zqC__tE9VCTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "sdkWBftOVEAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE Visualization Python\n",
        "\n",
        "We can now visualize the results by displaying two PCA components on a scatter plot.\n",
        "\n",
        "x: First component\n",
        "y: Second companion\n",
        "color: target variable.\n",
        "We have also used the update_layout function to add a title and rename the x-axis and y-axis."
      ],
      "metadata": {
        "id": "9axHV8dLVHht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(x=X_pca[:, 0], y=X_pca[:, 1], color=y)\n",
        "fig.update_layout(\n",
        "    title=\"PCA visualization of Custom Classification dataset\",\n",
        "    xaxis_title=\"First Principal Component\",\n",
        "    yaxis_title=\"Second Principal Component\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "AT-CnN6DVKXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitting and Transforming t-SNE\n",
        "\n",
        "Now we will apply the t-SNE algorithm to the dataset and compare the results.\n",
        "\n",
        "After fitting and transforming data, we will display Kullback-Leibler (KL) divergence between the high-dimensional probability distribution and the low-dimensional probability distribution.\n",
        "\n",
        "Low KL divergence is a sign of better results."
      ],
      "metadata": {
        "id": "SRsL0CV4VMP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "tsne.kl_divergence_"
      ],
      "metadata": {
        "id": "SFXjBM9mVPT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(x=X_tsne[:, 0], y=X_tsne[:, 1], color=y)\n",
        "fig.update_layout(\n",
        "    title=\"t-SNE visualization of Custom Classification dataset\",\n",
        "    xaxis_title=\"First t-SNE\",\n",
        "    yaxis_title=\"Second t-SNE\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "5_l0gJYqVQkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE on Customer Churn Dataset\n",
        "\n",
        "In this section, we will use the real Customer Churn dataset of an Iranian telecom company. The dataset contains information on the customers' activity, such as call failures and subscription length, and a churn label.\n",
        "\n",
        "Churn means the percentage of customers that stop using a particular service during a given time frame.\n",
        "\n",
        "Note: Code source and dataset from both examples are available at DataCamp Workspace.\n",
        "\n",
        "Importing Customer Churn Dataset\n",
        "\n",
        "We will load the dataset using pandas and display the first three rows."
      ],
      "metadata": {
        "id": "MccahmW7VUPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"data/customer_churn.csv\")\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "yCz_bq0sVXzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA Dimensionality Reduction\n",
        "\n",
        "After that, we will:\n",
        "\n",
        "Create features (X) and target (y) using the Churn column.\n",
        "Normalize the features using a standard scaler.\n",
        "Split the dataset into a training and testing set.\n",
        "Apply PCA to the training dataset.\n",
        "Get the score using the testing dataset. The score represents the average log-likelihood of all samples."
      ],
      "metadata": {
        "id": "XNHl2bgRVZeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_norm = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_norm, y, random_state=13, test_size=0.25, shuffle=True\n",
        ")\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "\n",
        "pca.score(X_test)"
      ],
      "metadata": {
        "id": "102QFyMqVdXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(x=X_train_pca[:, 0], y=X_train_pca[:, 1], color=y_train)\n",
        "fig.update_layout(\n",
        "    title=\"PCA visualization of Customer Churn dataset\",\n",
        "    xaxis_title=\"First Principal Component\",\n",
        "    yaxis_title=\"Second Principal Component\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "oO18QRLyVim2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "perplexity = np.arange(5, 55, 5)\n",
        "divergence = []\n",
        "\n",
        "for i in perplexity:\n",
        "    model = TSNE(n_components=2, init=\"pca\", perplexity=i)\n",
        "    reduced = model.fit_transform(X_train)\n",
        "    divergence.append(model.kl_divergence_)\n",
        "fig = px.line(x=perplexity, y=divergence, markers=True)\n",
        "fig.update_layout(xaxis_title=\"Perplexity Values\", yaxis_title=\"Divergence\")\n",
        "fig.update_traces(line_color=\"red\", line_width=1)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "k1ymLm5OVldS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZL9hLk8JVpMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE Dimensionality Reduction\n",
        "\n",
        "We will now fit t-SNE and transform the data into lower dimensions using 40 perplexity to get the lowest KL Divergence."
      ],
      "metadata": {
        "id": "IgWbxterVpOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2,perplexity=40, random_state=42)\n",
        "X_train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "tsne.kl_divergence_"
      ],
      "metadata": {
        "id": "QWSB4YUpVqXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(x=X_train_tsne[:, 0], y=X_train_tsne[:, 1], color=y_train)\n",
        "fig.update_layout(\n",
        "    title=\"t-SNE visualization of Customer Churn dataset\",\n",
        "    xaxis_title=\"First t-SNE\",\n",
        "    yaxis_title=\"Second t-SNE\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "o3Hb2Vb4VrzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Application of t-SNE\n",
        "\n",
        "Apart from visualizing complex multi-dimensional data, t-SNE has other uses mostly in the medical field.\n",
        "\n",
        "Clustering and classification: to cluster similar data points together in lower dimensional space. It can also be used for classification and finding patterns in the data.\n",
        "Anomaly detection: to identify outliers and anomalies in the data.\n",
        "Natural language processing: to visualize word embeddings generated from a large corpus of text that makes it easier to identify similarities and relationships between words.\n",
        "Computer security: to visualize network traffic patterns and detect anomalies.\n",
        "Cancer research: to visualize molecular profiles of tumor samples and identify subtypes of cancer.\n",
        "Geological domain interpretation: to visualize seismic attributes and to identify geological anomalies.\n",
        "Biomedical signal processing: to visualize electroencephalogram (EEG) and detect patterns of brain activity."
      ],
      "metadata": {
        "id": "7-MifTAZVu60"
      }
    }
  ]
}