{"cells":[{"source":"# Using XGBoost in Python Tutorial \n\nDiscover the power of XGBoost, one of the most popular machine learning frameworks among data scientists, with this step-by-step tutorial in Python.","metadata":{},"id":"c893d81f-57b5-43a2-8b50-2154c6dcfb4e","cell_type":"markdown"},{"source":"XGBoost is one of the most popular machine learning frameworks among data scientists. According to the Kaggle [State of Data Science Survey 2021](https://www.kaggle.com/kaggle-survey-2021), almost 50% of respondents said they used XGBoost, ranking below only TensorFlow and Sklearn. \n\n\n\nhttps://www.kaggle.com/kaggle-survey-2021\n\nThis XGBoost tutorial will introduce the key aspects of this popular Python framework, exploring how you can use it for your own machine learning projects.\n\n## What You Will Learn in This Python XGBoost Tutorial\n\nThroughout this tutorial, we will cover the key aspects of XGBoost, including:\n\n- Installation\n- XGBoost DMatrix class\n- XGBoost regression\n- Objective and loss functions in XGBoost\n- Building training and evaluation loops\n- Cross-validation in XGBoost\n- Building an XGBoost classifier\n- Changing between Sklearn and native APIs of XGBoost\n\n\nLet’s get started!","metadata":{},"id":"45706eb0-21cb-4989-b755-1207c3ce5078","cell_type":"markdown"},{"source":"## Loading and Exploring the Data\n\nWe will be working with the Diamonds dataset throughout the tutorial. It is built into the Seaborn library, or alternatively, you can also [download it from Kaggle](https://www.kaggle.com/datasets/shivam2503/diamonds). It has a nice combination of numeric and categorical features and over 50k observations that we can comfortably showcase all the advantages of XGBoost.","metadata":{},"id":"a5cc777c-14e0-4219-8fa5-4cb51bf26720","cell_type":"markdown"},{"source":"import seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\ndiamonds = sns.load_dataset(\"diamonds\")\ndiamonds.head()","metadata":{},"id":"697f5e24-8c4e-4e1c-a8ed-7a2498e2d12b","cell_type":"code","execution_count":null,"outputs":[]},{"source":"diamonds.shape","metadata":{},"id":"1c023821-ad93-44a6-a4e5-997a6c3da7c6","cell_type":"code","execution_count":null,"outputs":[]},{"source":"In a typical real-world project, you would want to spend a lot more time exploring the dataset and visualizing its features. But since this data comes built-in to Seaborn, it is relatively clean.\n\nSo, we will just look at the 5-number summary of the numeric and categorical features and get going. You can spend a few moments to familiarize yourself with the dataset.","metadata":{},"id":"8766bb0b-2365-425d-8d35-3632763f5ffc","cell_type":"markdown"},{"source":"diamonds.describe()","metadata":{},"id":"b994ba2e-d1b7-42ea-971b-99bbcc1ba7d8","cell_type":"code","execution_count":null,"outputs":[]},{"source":"diamonds.describe(exclude=np.number)","metadata":{},"id":"ba8f9655-0378-4d6d-a4cb-03b0887aa350","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## How to Build an XGBoost DMatrix\n\nAfter you are done with exploration, the first step in any project is framing the machine learning problem and extracting the feature and target arrays based on the dataset.\n\nIn this tutorial, we will first try to predict diamond prices using their physical measurements, so our target will be the price column.\n\nSo, we are isolating the features into X and the target into y:","metadata":{},"id":"85b47ea7-6a2d-4b19-abb7-8beb9556d9e8","cell_type":"markdown"},{"source":"from sklearn.model_selection import train_test_split\n\n# Extract feature and target arrays\nX, y = diamonds.drop('price', axis=1), diamonds[['price']]","metadata":{},"id":"0553fabb-32c0-4e73-8457-1c6b66af9f96","cell_type":"code","execution_count":null,"outputs":[]},{"source":"The dataset has three categorical columns. Normally, you would encode them with ordinal or one-hot encoding, but XGBoost has the ability to internally deal with categoricals.\n\nThe way to enable this feature is to cast the categorical columns into Pandas `category` data type (by default, they are treated as text columns):","metadata":{},"id":"8ca4903b-222b-4811-ac0f-0be0913fa9ba","cell_type":"markdown"},{"source":"# Extract text features\ncats = X.select_dtypes(exclude=np.number).columns.tolist()\n\n# Convert to Pandas category\nfor col in cats:\n   X[col] = X[col].astype('category')","metadata":{},"id":"879a00bc-75c6-4c52-ae53-1cd8a4774e2d","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Now, when you print the `dtypes` attribute, you'll see that we have three `category` features:","metadata":{},"id":"051c9124-eac6-440c-b4c7-ce9ba85cdd5a","cell_type":"markdown"},{"source":"X.dtypes","metadata":{},"id":"57a3022f-4522-4f26-9533-657df5ccf8e8","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Let’s split the data into train, and test sets (0.25 test size):","metadata":{},"id":"b8e524cf-12e6-42e4-9749-dc1ed1db20ba","cell_type":"markdown"},{"source":"# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)","metadata":{},"id":"84f3aaab-994b-436f-9ede-ef2ec43f3824","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Now, the important part: XGBoost comes with its own class for storing datasets called DMatrix. It is a highly optimized class for memory and speed. That's why converting datasets into this format is a requirement for the native XGBoost API:","metadata":{},"id":"00374769-7a7d-407d-979b-6bc78bb8b4ab","cell_type":"markdown"},{"source":"import xgboost as xgb\n\n# Create regression matrices\ndtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\ndtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)","metadata":{},"id":"02691e29-7fbd-40c4-ba6f-968bb846cd02","cell_type":"code","execution_count":null,"outputs":[]},{"source":"The class accepts both the training features and the labels. To enable automatic encoding of Pandas category columns, we also set `enable_categorical` to True.\n\n**Note**:\n\nWhy are we going with the native API of XGBoost, rather than its Scikit-learn API? While it might be more comfortable to use the Sklearn API at first, later, you’ll realize that the native API of XGBoost contains some excellent features that the former doesn’t support. So, better get used to it from the beginning. However, there is a section at the end where we show how to switch between APIs in a single line of code even after you have trained models.\n\n## Python XGBoost Regression\n\nAfter building the DMatrices, you should choose a value for the `objective` parameter. It tells XGBoost the machine learning problem you are trying to solve and what metrics or loss functions to use to solve that problem.\n\nFor example, to predict diamond prices, which is a regression problem, you can use the common `reg:squarederror` objective. Usually, the name of the objective also contains the name of the loss function for the problem. For regression, it is common to use Root Mean Squared Error, which minimizes the square root of the squared sum of the differences between actual and predicted values. Here is how the metric would look like when implemented in NumPy:\n\n```python\nimport numpy as np\n\nmse = np.mean((actual - predicted) ** 2)\nrmse = np.sqrt(mse)\n```\n\nWe’ll learn classification objectives later in the tutorial.\n\nA note on the difference between a loss function and a performance metric: A loss function is used by machine learning models to minimize the _differences_ between the actual (ground truth) values and model predictions. On the other hand, a **metric** (or metrics) is chosen by the machine learning engineer to measure the _similarity_ between ground truth and model predictions.\n\nIn short, a loss function should be minimized while a metric should be maximized. A loss function is used during training to guide the model on where to improve. A metric is used during evaluation to measure overall performance.\n\n### Training\n\nThe chosen objective function and any other hyperparameters of XGBoost should be specified in a dictionary, which by convention should be called params:","metadata":{},"id":"c1ebcda0-8406-4cb7-8691-a7c7d3cb30ca","cell_type":"markdown"},{"source":"# Define hyperparameters\nparams = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"}\n\n# In case you have access to a GPU\n# params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}","metadata":{},"id":"d9e14817-1b13-4d96-93c9-e8154b8f3101","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Now, we set another parameter called `num_boost_round`, which stands for _number_ _of boosting rounds_. Internally, XGBoost minimizes the loss function RMSE in small incremental rounds (more on this later). This parameter specifies the amount of those rounds.\n\nThe ideal number of rounds is found through hyperparameter tuning. For now, we will just set it to 100:","metadata":{},"id":"cafd764b-9dec-4723-871f-ba7ff54ebb87","cell_type":"markdown"},{"source":"n = 100\nmodel = xgb.train(\n   params=params,\n   dtrain=dtrain_reg,\n   num_boost_round=n,\n)","metadata":{},"id":"d559836a-bc4d-486c-b655-613652d3a473","cell_type":"code","execution_count":null,"outputs":[]},{"source":"When XGBoost runs on a GPU, it is blazing fast. If you didn’t receive any errors from the above code, the training was successful!\n\n### Evaluation\n\nDuring the boosting rounds, the model object has learned all the patterns of the training set it possibly can. Now, we must measure its performance by testing it on unseen data. That's where our `dtest_reg` DMatrix comes into play:","metadata":{},"id":"3ca4535d-b0bd-4605-bad4-dd010ad300d0","cell_type":"markdown"},{"source":"from sklearn.metrics import mean_squared_error\n\npreds = model.predict(dtest_reg)","metadata":{},"id":"5133e0d5-0bba-4b3a-98ec-1c216d35d8a1","cell_type":"code","execution_count":null,"outputs":[]},{"source":"This step of the process is called model evaluation (or inference). Once you generate predictions with predict, you pass them inside `mean_squared_error` function of Sklearn to compare against `y_test`:","metadata":{},"id":"421c4fcb-113e-420b-9136-6030d33e5fc6","cell_type":"markdown"},{"source":"rmse = mean_squared_error(y_test, preds, squared=False)\n\nprint(f\"RMSE of the base model: {rmse:.3f}\")","metadata":{},"id":"4119ac45-fcb7-4e21-aafa-2f86277e8189","cell_type":"code","execution_count":null,"outputs":[]},{"source":"We’ve got a base score ~543$, which was the performance of a base model with default parameters. There are two ways we can improve it— by performing cross-validation and hyperparameter tuning. But before that, let’s see a quicker way of evaluating XGBoost models.\n\n## Using Validation Sets During Training\n\nTraining a machine learning model is like launching a rocket into space. You can control everything about the model up to the launch, but once it does, all you can do is stand by and wait for it to finish.\n\nBut the problem with our current training process is that we can’t even watch where the model is going. To solve this, we will use evaluation arrays that allow us to see model performance as it gets improved incrementally across boosting rounds.\n\nFirst, let’s set up the parameters again:","metadata":{},"id":"629f9658-1523-4a27-954f-971d89ff9fd1","cell_type":"markdown"},{"source":"params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"}\nn = 100","metadata":{},"id":"7d187a35-c045-4aa0-bbb6-3bb77584a63d","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Next, we create a list of two tuples that each contain two elements. The first element is the array for the model to evaluate, and the second is the array’s name.","metadata":{},"id":"3ddfbc6d-bf88-41bb-8a0e-98c9c6b89471","cell_type":"markdown"},{"source":"evals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]","metadata":{},"id":"b632539c-d982-46e3-a6a0-dc5aea07c149","cell_type":"code","execution_count":null,"outputs":[]},{"source":"When we pass this array to the `evals` parameter of `xgb.train`, we will see the model performance after each boosting round:","metadata":{},"id":"b658cfa6-e3c4-47a9-ab31-d9af29069902","cell_type":"markdown"},{"source":"model = xgb.train(\n   params=params,\n   dtrain=dtrain_reg,\n   num_boost_round=n,\n   evals=evals,\n)","metadata":{},"id":"a958e2ea-934a-41c3-84f2-925aad951080","cell_type":"code","execution_count":null,"outputs":[]},{"source":"You can see how the model minimizes the score from a whopping ~3931\\$ to just 543\\$.\n\nWhat’s best is that we can see the model’s performance on both our training and validation sets. Usually, the training loss will be lower than validation since the model has already seen the former.\n\nIn real-world projects, you usually train for thousands of boosting rounds, which means that many rows of output. To reduce them, you can use the `verbose_eval` parameter, which forces XGBoost to print performance updates every `vebose_eval` rounds:","metadata":{},"id":"04fe9797-ee79-4aa2-9c78-47e7042d2fc3","cell_type":"markdown"},{"source":"params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"}\nn = 100\n\nevals = [(dtest_reg, \"validation\"), (dtrain_reg, \"train\")]\n\n\nmodel = xgb.train(\n   params=params,\n   dtrain=dtrain_reg,\n   num_boost_round=n,\n   evals=evals,\n   verbose_eval=10 # Every ten rounds\n)","metadata":{},"id":"2e00b25b-50e5-49a1-8c60-01eb7b6f9eb0","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## XGBoost Early Stopping\n\nBy now, you must have realized how important boosting rounds are. Generally, the more rounds there are, the more XGBoost tries to minimize the loss. But this doesn’t mean the loss will always go down. Let’s try with 5000 boosting rounds with the verbosity of 500:","metadata":{},"id":"d076a028-b0d7-4255-ba93-0476fe83f69a","cell_type":"markdown"},{"source":"params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"}\nn = 5000\n\nevals = [(dtest_reg, \"validation\"), (dtrain_reg, \"train\")]\n\n\nmodel = xgb.train(\n   params=params,\n   dtrain=dtrain_reg,\n   num_boost_round=n,\n   evals=evals,\n   verbose_eval=250\n)","metadata":{},"id":"3b2f6516-950a-4015-a17f-82d795f48970","cell_type":"code","execution_count":null,"outputs":[]},{"source":"We get the lowest loss before round 500. After that, even though training loss keeps going down, the validation loss (the one we care about) keeps increasing.\n\nWhen given an unnecessary number of boosting rounds, XGBoost starts to overfit and memorize the dataset. This, in turn, leads to validation performance drop because the model is memorizing instead of generalizing.\n\nRemember, we want the **golden middle**: a model that learned just enough patterns in training that it gives the highest performance on the validation set. So, how do we find the perfect number of boosting rounds, then?\n\nWe will use a technique called **early stopping**. Early stopping forces XGBoost to watch the validation loss, and if it stops improving for a specified number of rounds, it automatically stops training.\n\nThis means we can set as high a number of boosting rounds as long as we set a sensible number of early stopping rounds.\n\nFor example, let’s use 10000 boosting rounds and set the `early_stopping_rounds` parameter to 50. This way, XGBoost will automatically stop the training if validation loss doesn't improve for 50 consecutive rounds.","metadata":{},"id":"c977433f-443d-47ce-b5f0-7691e381d8c9","cell_type":"markdown"},{"source":"n = 5000\n\nmodel = xgb.train(\n   params=params,\n   dtrain=dtrain_reg,\n   num_boost_round=n,\n   evals=evals,\n   verbose_eval=100,\n   # Activate early stopping\n   early_stopping_rounds=50\n)","metadata":{},"id":"c1dc4613-d058-43e9-986a-715349e08360","cell_type":"code","execution_count":null,"outputs":[]},{"source":"As you can see, the training stopped after the 167th round because the loss stopped improving for 50 rounds before that.\n\n## XGBoost Cross-Validation\n\nAt the beginning of the tutorial, we set aside 25% of the dataset for testing. The test set would allow us to simulate the conditions of a model in production, where it must generate predictions for unseen data.\n\nBut only a single test set would not be enough to measure how a model would perform in production accurately. For example, if we perform hyperparameter tuning using only a single training and a single test set, knowledge about the test set would still “leak out.” How?\n\nSince we try to find the best value of a hyperparameter by comparing the validation performance of the model on the test set, we will end up with a model that is configured to perform well _only_ on that particular test set. Instead, we want a model that performs well across the board — on any test set we throw at it.\n\nA possible workaround is splitting the data into three sets. The model trains on the first set, the second set is used for evaluation and hyperparameter tuning, and the third is the final one we test the model before production.\n\nBut when data is limited, splitting data into three sets will make the training set sparse, which hurts model performance.\n\nThe solution to all these problems is cross-validation. In cross-validation, we still have two sets: training and testing.\n\nWhile the test set waits in the corner, we split the training into 3, 5, 7, or _k_ splits or folds. Then, we train the model _k_ times. Each time, we use _k-1_ parts for training and the final _k_th part for validation. This process is called k-fold cross-validation:\n\n\n\nSource: https://scikit-learn.org/stable/modules/cross_validation.html\n\nAbove is a visual depiction of a 5-fold cross-validation. After all folds are done, we can take the mean of the scores as the final, most realistic performance of the model.\n\nLet’s perform this process in code using the `cv` function of XGB:","metadata":{},"id":"560d95bf-c266-498c-9654-bf39aaed8cba","cell_type":"markdown"},{"source":"params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"}\nn = 1000\n\nresults = xgb.cv(\n   params, dtrain_reg,\n   num_boost_round=n,\n   nfold=5,\n   early_stopping_rounds=20\n)","metadata":{},"id":"aa4dd401-a11e-4c8f-84ec-92766b6d0003","cell_type":"code","execution_count":null,"outputs":[]},{"source":"The only difference with the train function is adding the `nfold` parameter to specify the number of splits. The results object is now a DataFrame containing each fold's results:","metadata":{},"id":"c2fce8e3-d4fe-4c42-a3cc-e0be225f985c","cell_type":"markdown"},{"source":"results.head()","metadata":{},"id":"4314b768-75af-43cf-ba18-3b68c2bf89cc","cell_type":"code","execution_count":null,"outputs":[]},{"source":"It has the same number of rows as the number of boosting rounds. Each row is the average of all splits for that round. So, to find the best score, we take the minimum of the `test-rmse-mean` column:","metadata":{},"id":"02d171f0-9d54-4844-b7f8-c5751e23fdc3","cell_type":"markdown"},{"source":"best_rmse = results['test-rmse-mean'].min()\nbest_rmse","metadata":{},"id":"95805caf-7671-4c4d-8861-20d960602c11","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Note that this method of cross-validation is used to see the true performance of the model. Once satisfied with its score, you must retrain it on the full data before deployment.\n\n## XGBoost Classification\n\nBuilding an XGBoost classifier is as easy as changing the objective function; the rest can stay the same.\n\nThe two most popular classification objectives are:\n\n- `binary:logistic` - binary classification (the target contains only two classes, i.e., cat or dog)\n- `multi:softprob` - multi-class classification (more than two classes in the target, i.e., apple/orange/banana)\n\n\nPerforming binary and multi-class classification in XGBoost is almost identical, so we will go with the latter. Let’s prepare the data for the task first.\n\nWe want to predict the cut quality of diamonds given their price and their physical measurements. So, we will build the feature/target arrays accordingly:","metadata":{},"id":"e6484ebc-8789-4308-b1fc-be56bcbfa4fd","cell_type":"markdown"},{"source":"from sklearn.preprocessing import OrdinalEncoder\n\nX, y = diamonds.drop(\"cut\", axis=1), diamonds[['cut']]\n\n# Encode y to numeric\ny_encoded = OrdinalEncoder().fit_transform(y)\n\n# Extract text features\ncats = X.select_dtypes(exclude=np.number).columns.tolist()\n\n# Convert to pd.Categorical\nfor col in cats:\n   X[col] = X[col].astype('category')\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=1, stratify=y_encoded)","metadata":{},"id":"7eaece62-2047-403b-a7ad-e85b23cb4510","cell_type":"code","execution_count":null,"outputs":[]},{"source":"The only difference is that since XGBoost only accepts numbers in the target, we are encoding the text classes in the target with `OrdinalEncoder` of Sklearn.\n\nNow, we build the DMatrices…","metadata":{},"id":"a30bf623-cb0d-4c5f-bde0-dc3063c385c4","cell_type":"markdown"},{"source":"# Create classification matrices\ndtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)\ndtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)","metadata":{},"id":"c9befaf0-ec23-4d26-949e-528055ee574b","cell_type":"code","execution_count":null,"outputs":[]},{"source":"…and set the objective to `multi:softprob`. This objective also requires the number of classes to be set by us:","metadata":{},"id":"782af288-07eb-44e0-bbc8-dc250be4b8df","cell_type":"markdown"},{"source":"params = {\"objective\": \"multi:softprob\", \"tree_method\": \"hist\", \"num_class\": 5}\nn = 1000\n\nresults = xgb.cv(\n   params, dtrain_clf,\n   num_boost_round=n,\n   nfold=5,\n   metrics=[\"mlogloss\", \"auc\", \"merror\"],\n)","metadata":{},"id":"facfe124-1714-40d6-aece-a326b94d9b1d","cell_type":"code","execution_count":null,"outputs":[]},{"source":"During cross-validation, we are asking XGBoost to watch three classification metrics which report model performance from three different angles. Here is the result:","metadata":{},"id":"6f7e82bc-2bed-4340-8450-511a1b4e00be","cell_type":"markdown"},{"source":"results.keys()","metadata":{},"id":"8d5b6b10-4942-4a23-870f-8cdf9e2e3e54","cell_type":"code","execution_count":null,"outputs":[]},{"source":"To see the best AUC score, we take the maximum of test-auc-mean column:","metadata":{},"id":"8cac1535-ea02-436a-ad34-a36e90a167b5","cell_type":"markdown"},{"source":"results['test-auc-mean'].max()","metadata":{},"id":"42e66f7f-57c3-4500-8cf3-6db55f4c2691","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Even the default configuration gave us 94% performance, which is great.\n\n## XGBoost Native vs. XGBoost Sklearn\n\nSo far, we have been using the native XGBoost API, but its Sklearn API is pretty popular as well.\n\nSklearn is a vast framework with many machine learning algorithms and utilities and has an API syntax loved by almost everyone. Therefore, XGBoost also offers XGBClassifier and XGBRegressor classes so that they can be integrated into the Sklearn ecosystem (at the loss of some of the functionality).\n\nIf you want to only use the Scikit-learn API whenever possible and only switch to native when you need access to extra functionality, there is a way.\n\nAfter training the XGBoost classifier or regressor, you can convert it using the `get_booster` method:","metadata":{},"id":"7de5b838-9e3a-48a9-925e-b0ca216daf03","cell_type":"markdown"},{"source":"import xgboost as xgb\n\n# Train a model using the scikit-learn API\nxgb_classifier = xgb.XGBClassifier(n_estimators=100, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=3, enable_categorical=True)\nxgb_classifier.fit(X_train, y_train)\n\n# Convert the model to a native API model\nmodel = xgb_classifier.get_booster()","metadata":{},"id":"7a0b2588-79b7-44ef-98b4-bbe7e06d6f5b","cell_type":"code","execution_count":null,"outputs":[]},{"source":"The model object will behave in the exact same way we've seen throughout this tutorial.\n\n## Conclusion\n\nWe’ve covered a lot of important topics in this XGBoost tutorial, but there are still so many things to learn.\n\nYou can check out the[ XGBoost parameters page](https://xgboost.readthedocs.io/en/stable/parameter.html), which teaches you how to configure the parameters to squeeze out every last performance from your models.\n\nIf you are looking for a comprehensive, all-in-one resource to learn the library, check out our [Extreme Gradient Boosting With XGBoost course](https://www.datacamp.com/courses/extreme-gradient-boosting-with-xgboost).","metadata":{},"id":"8ec28960-fadc-479f-9bef-b7325ce6bab1","cell_type":"markdown"}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}