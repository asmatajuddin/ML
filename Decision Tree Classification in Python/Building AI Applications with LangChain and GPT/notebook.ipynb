{"cells":[{"source":"# Live training | 2023-06-13 | Building AI Applications with LangChain and GPT","metadata":{},"id":"bcd52f5b-d79b-4e61-a44d-aaa20e720380","cell_type":"markdown"},{"source":"You've probably talked to ChatGPT using the web interface, or used the API with the `openai` python package and wondered \"what if I could teach it about my own data?\". Today we're going to build such an application using LangChain, a framework for developing applications powered by language models.\n\nIn today's session, we'll build a chatbot powered by GPT-3.5 that can answer questions about LangChain, as it will have knowledge of the LangChain documentation. We'll cover:\n- Getting setup with an OpenAI developer account and integration with Workspace;\n- Install the LangChain package\n- Preparing the data\n- Embed the data using OpenAI's Embed API, and get a cost estimate for this operation\n- Storing the data in a vector database\n- How to query the vector database\n- Putting together a basic chat application to \"talk to the LangChain docs\"","metadata":{},"id":"42874dfa-76fc-4622-b2ab-1bbdd6c7f4f0","cell_type":"markdown"},{"source":"## Before you begin","metadata":{},"id":"e41be986-bad8-497c-80f8-11b50a7273f7","cell_type":"markdown"},{"source":"### Unzip the required data by running the following cell","metadata":{},"id":"d287b194-3f17-4a83-b39e-7b1a5fe0620e","cell_type":"markdown"},{"source":"!test -f contents.zip && unzip contents.zip && rm contents.zip","metadata":{},"id":"137ee71e-0ac1-47d9-973c-9320ce7d47f2","cell_type":"code","execution_count":null,"outputs":[]},{"source":"### Create a developer account with OpenAI\n\n1. Go to the [API signup page](https://platform.openai.com/signup). \n\n2. Create your account (you'll need to provide your email address and your phone number).\n\n<img src=\"images/openai-create-account.jpeg\" width=\"200\">\n\n3. Go to the [API keys page](https://platform.openai.com/account/api-keys). \n\n4. Create a new secret key.\n\n<img src=\"images/openai-new-secret-key.png\" width=\"200\">\n\n5. **Take a copy of it**. (If you lose it, delete the key and create a new one.)","metadata":{},"id":"53a1012a-683a-40d0-8a36-da6df3b23154","cell_type":"markdown"},{"source":"### Add a payment method\n\nOpenAI sometimes provides free credits for the API, but it's not clear if that is worldwide or what the conditions are. You may need to add debit/credit card details. \n\nWe will use 2 APIs:\n- The Chat API with the `gpt-3.5-turbo` model (cost: $0.002 / 1K tokens)\n- The Embedding API with the `Ada v2` model (cost: $0.0004 / 1K tokens)\n\n**In total, the Chat API (used for completions) should cost less than `$0.1` and embedding should cost around `$0.6`. This notebook provides embeddings already, so you can skip the embedding step.**\n\n1. Go to the [Payment Methods page](https://platform.openai.com/account/billing/payment-methods).\n\n2. Click Add payment method.\n\n<img src=\"images/openai-add-payment-method.png\" width=\"200\">\n\n3. Fill in your card details.","metadata":{},"id":"8ee72ba1-5455-44df-9931-7bb7d4e407bf","cell_type":"markdown"},{"source":"### Set up Environment Variables\n\n1. In Workspace, click on Environment.\n\n<img src=\"images/workspace-environment.png\" width=\"200\">\n\n2. Click on the \"Environment Variables\" plus button.\n\n<img src=\"images/workspace-new-env-vars.png\" width=\"200\">\n\n3. In the \"Name\" field, type `OPENAI_API_KEY`. In the \"Value\" field, paste in your secret key (starting with `sk-`)\n\n<img src=\"images/workspace-create-env-vars.png\" width=\"300\">\n\n4. Click \"Create\", and connect the new integration.\n\n<img src=\"images/workspace-connect-env-vars.png\" width=\"300\">\n","metadata":{},"id":"415516c6-0c44-4409-9e7e-e683ab1cfbb2","cell_type":"markdown"},{"source":"## Task 0: Setup\n\nFor the purpose of this training, we'll need to install a few packages:\n- [`langchain`](https://python.langchain.com/en/latest/index.html): The LangChain framework\n- [`chromadb`](https://docs.trychroma.com/): The package we'll use for the vector database\n- [`tiktoken`](https://github.com/openai/tiktoken): A tokenizer we'll use to count GPT-3 tokens","metadata":{},"id":"eb20b7a5-5459-440d-af85-bc83704873ac","cell_type":"markdown"},{"source":"# install langchain (version 0.0.191)\n!pip install langchain==0.0.191\n# install chromadb\n!pip install chromadb==0.3.26\n# install tiktoken\n!pip install tiktoken==0.4.0\n","metadata":{"executionCancelledAt":null,"executionTime":13274,"lastExecutedAt":1686668947683,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# install langchain (version 0.0.191)\n!pip install langchain==0.0.191\n# install chromadb\n!pip install chromadb\n# install tiktoken\n!pip install tiktoken\n","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"c29e2ef0-c6f2-4ce2-9c02-6c00f9cd155a","cell_type":"code","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: langchain==0.0.191 in /home/repl/.local/lib/python3.8/site-packages (0.0.191)\nRequirement already satisfied: PyYAML>=5.4.1 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.0.191) (6.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.191) (1.4.40)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.0.191) (3.8.4)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.191) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.0.191) (0.5.8)\nRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.0.191) (2.8.4)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.191) (1.23.2)\nRequirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.0.191) (1.2.4)\nRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.191) (1.10.2)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.191) (2.28.1)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.0.191) (8.2.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.191) (21.4.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.191) (2.0.12)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.191) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.191) (1.8.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.191) (1.3.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.191) (1.2.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/repl/.local/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.191) (3.19.0)\nRequirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/repl/.local/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.191) (1.5.1)\nRequirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.191) (0.8.0)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<2,>=1->langchain==0.0.191) (4.5.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.0.191) (2.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/repl/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain==0.0.191) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.0.191) (2019.11.28)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.191) (1.1.3)\nRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.191) (21.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.191) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.191) (3.0.9)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: chromadb in /home/repl/.local/lib/python3.8/site-packages (0.3.26)\nRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.8/dist-packages (from chromadb) (1.5.1)\nRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.8/dist-packages (from chromadb) (2.28.1)\nRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.8/dist-packages (from chromadb) (1.10.2)\nRequirement already satisfied: hnswlib>=0.7 in /home/repl/.local/lib/python3.8/site-packages (from chromadb) (0.7.0)\nRequirement already satisfied: clickhouse-connect>=0.5.7 in /home/repl/.local/lib/python3.8/site-packages (from chromadb) (0.6.2)\nRequirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from chromadb) (0.8.0)\nRequirement already satisfied: fastapi>=0.85.1 in /home/repl/.local/lib/python3.8/site-packages (from chromadb) (0.97.0)\nRequirement already satisfied: uvicorn[standard]>=0.18.3 in /home/repl/.local/lib/python3.8/site-packages (from chromadb) (0.22.0)\nRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.8/dist-packages (from chromadb) (1.23.2)\nRequirement already satisfied: posthog>=2.4.0 in /home/repl/.local/lib/python3.8/site-packages (from chromadb) (3.0.1)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.8/dist-packages (from chromadb) (4.5.0)\nRequirement already satisfied: pulsar-client>=3.1.0 in /home/repl/.local/lib/python3.8/site-packages (from chromadb) (3.2.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /home/repl/.local/lib/python3.8/site-packages (from chromadb) (1.15.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.8/dist-packages (from chromadb) (0.13.2)\nRequirement already satisfied: tqdm>=4.65.0 in /home/repl/.local/lib/python3.8/site-packages (from chromadb) (4.65.0)\nRequirement already satisfied: overrides>=7.3.1 in /home/repl/.local/lib/python3.8/site-packages (from chromadb) (7.3.1)\nRequirement already satisfied: graphlib-backport>=1.0.3 in /home/repl/.local/lib/python3.8/site-packages (from chromadb) (1.0.3)\nRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2019.11.28)\nRequirement already satisfied: urllib3>=1.26 in /home/repl/.local/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.16)\nRequirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.7)\nRequirement already satisfied: zstandard in /home/repl/.local/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.21.0)\nRequirement already satisfied: lz4 in /home/repl/.local/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/repl/.local/lib/python3.8/site-packages (from fastapi>=0.85.1->chromadb) (0.27.0)\nRequirement already satisfied: coloredlogs in /home/repl/.local/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime>=1.14.1->chromadb) (21.3)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.19.4)\nRequirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.11)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from posthog>=2.4.0->chromadb) (1.14.0)\nRequirement already satisfied: monotonic>=1.5 in /home/repl/.local/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb) (1.6)\nRequirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from posthog>=2.4.0->chromadb) (2.1.2)\nRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->chromadb) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.28->chromadb) (2.8)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\nRequirement already satisfied: h11>=0.8 in /home/repl/.local/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /home/repl/.local/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.5.0)\nRequirement already satisfied: python-dotenv>=0.13 in /home/repl/.local/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\nRequirement already satisfied: pyyaml>=5.1 in /home/repl/.local/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/repl/.local/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\nRequirement already satisfied: watchfiles>=0.13 in /home/repl/.local/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: websockets>=10.4 in /home/repl/.local/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\nRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (3.6.1)\nRequirement already satisfied: humanfriendly>=9.1 in /home/repl/.local/lib/python3.8/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->onnxruntime>=1.14.1->chromadb) (3.0.9)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.2.1)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.2.0)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: tiktoken in /home/repl/.local/lib/python3.8/site-packages (0.4.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.8/dist-packages (from tiktoken) (2022.8.17)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.8/dist-packages (from tiktoken) (2.28.1)\nRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/repl/.local/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2019.11.28)\n"}]},{"source":"## Task 1: Load data\n\nTo be able to embed and store data, we need to provide LangChain with Documents. This is easy to achieve in LangChain thanks to [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html). In our case, we're targeting a \"Read the docs\" documentation, for which there is a loader [`ReadTheDocsLoader`](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/readthedocs_documentation.html).\nIn the folder `rtdocs`, you'll find all the HTML files from the LangChain documentation (https://python.langchain.com/en/latest/index.html). \n\n<details>\n    <summary>How did we obtain the data</summary>\n<br/><p>These file were downloaded by executing this linux command:</p>\n<pre>\nwget -r -A.html -P rtdocs https://python.langchain.com/en/latest/\n</pre>\n<p>We urge you **NOT** to execute this during the live training, as it will scan and download the full langchain doc site (~1000 files). This operation may be heavy and could disrupt the site, especially if hundreds of learners do it all at once!</p>\n</details>\n<br/><br/>\n\n\nOur first task is to load these HTML files as documents that we can use with langchain: we're going to use the `ReadTheDocsLoader`. It will read the directory containing all HTML files and transform them into `Document` objects. `ReadTheDocsLoader` will read each HTML file, remove HTML tags to only keep the text and return it as a `Document`. At the end of this task, we'll have a variable `raw_documents` containing a list of `Document`: one `Document` per HTML file. \n\nNote that in this step we won't actually load the documents into a database, we're simply loading the documents in a list.\n\n### Instructions\n1. import `ReadTheDocsLoader` from `langchain.document_loaders`\n2. Create the loader, pointing to the `rtdocs/python.langchain.com/en/latest` directory and enabling the HTML parser feature with `features='html.parser'`\n3. Load the data in `raw_documents` by calling `loader.load()`\n","metadata":{},"id":"46576543-af99-4a33-8dd1-fc01d5e476e4","cell_type":"markdown"},{"source":"# Import ReadTheDocsLoader\nfrom langchain.document_loaders import ReadTheDocsLoader\n\n# Create a loader for the `rtdocs/python.langchain.com/en/latest` folder\nloader = ReadTheDocsLoader(\"rtdocs/python.langchain.com/en/latest\", features=\"html.parser\")\n\n# Load the data\nraw_documents = loader.load()","metadata":{"executionCancelledAt":null,"executionTime":113457,"lastExecutedAt":1686669901162,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import ReadTheDocsLoader\nfrom langchain.document_loaders import ReadTheDocsLoader\n\n# Create a loader for the `rtdocs/python.langchain.com/en/latest` folder\nloader = ReadTheDocsLoader(\"rtdocs/python.langchain.com/en/latest\", features=\"html.parser\")\n\n# Load the data\nraw_documents = loader.load()"},"id":"92e2087c-58cf-463f-9055-144a58d674a2","cell_type":"code","execution_count":22,"outputs":[]},{"source":"## Task 2: Slice the documents into smaller chunks\n\nIn the previous step, we turned each HTML file into a Document. These files may be very long, and are potentially too large to embed fully. It's also a good practice to avoid embedding large documents:\n- long documents often contain several concepts. Retrieval will be easier if each concept is indexed separately;\n- retrieved documents will be injected in a prompt, so keeping them short will keep the prompt small(ish)\n\nLangChain has a collection of tools to do this: [Text Splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html). In our case, we'll be using the most straightfoward one and simplest to use: the [Recursive Character Text Splitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html). The recursive text splitter will recursively reduce the input by splitting it by paragraph, then sentences, then words as needed until the chunk is small enough. \n\n### Instructions\n1. Import the `RecursiveCharacterTextSplitter` from `langchain.text_splitter`\n2. Create a text splitter configured with `chunk_size=1000` and `chunk_overlap=200`  \n   _These values are arbitrary and you'll need to try different ones to see which best serve your use case_\n3. split the `raw_documents` and store them as `documents`, using the `.split_documents()` method","metadata":{},"id":"49c09718-cddd-414b-a512-8c95d1cec69d","cell_type":"markdown"},{"source":"# Import RecursiveCharacterTextSplitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Create the text splitter\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\n\n# Split the documents\ndocuments = splitter.split_documents(raw_documents)","metadata":{"executionCancelledAt":null,"executionTime":345,"lastExecutedAt":1686670251826,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import RecursiveCharacterTextSplitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Create the text splitter\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\n\n# Split the documents\ndocuments = splitter.split_documents(raw_documents)"},"id":"dd571f4b-f8e6-474d-a837-7f0101f66c6f","cell_type":"code","execution_count":23,"outputs":[]},{"source":"documents[0]","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1686670291922,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"documents[0]"},"id":"5b21c3f5-a50c-4c19-a7ee-a3854e90226e","cell_type":"code","execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":"Document(page_content='.md\\n.pdf\\nConcepts\\n Contents \\nChain of Thought\\nAction Plan Generation\\nReAct\\nSelf-ask\\nPrompt Chaining\\nMemetic Proxy\\nSelf Consistency\\nInception\\nMemPrompt\\nConcepts#\\nThese are concepts and terminology commonly used when developing LLM applications.\\nIt contains reference to external papers or sources where the concept was first introduced,\\nas well as to places in LangChain where the concept is used.\\nChain of Thought#\\nChain of Thought (CoT) is a prompting technique used to encourage the model to generate a series of intermediate reasoning steps.\\nA less formal way to induce this behavior is to include â€œLetâ€™s think step-by-stepâ€ in the prompt.\\nChain-of-Thought Paper\\nStep-by-Step Paper\\nAction Plan Generation#\\nAction Plan Generation is a prompting technique that uses a language model to generate actions to take.\\nThe results of these actions can then be fed back into the language model to generate a subsequent action.\\nWebGPT Paper\\nSayCan Paper\\nReAct#', metadata={'source': 'rtdocs/python.langchain.com/en/latest/getting_started/concepts.html'})"},"metadata":{},"execution_count":26}]},{"source":"## Task 3: count tokens and get a cost estimate of embedding\n\nWe're now ready to embed our documents. Before we do so, we'd like to get an idea of how big it is and how much it will cost to embed. To do so, we'll use the [`tiktoken`](https://github.com/openai/tiktoken) library (no relation to TikTok, there is no dancing involved). tiktoken allows to encode and decode strings of text into tokens. In our case, we're mostly interested in how many tokens our documents translate to.\n\n> ðŸ’¡ To better understand what a token is to GPT, head to [OpenAI's Tokenizer page](https://platform.openai.com/tokenizer) where you can see how a text translates to tokens.\n\nPrices for different models can be found on their [pricing page](https://openai.com/pricing).\n\n### Instructions\n1. Import `tiktoken`\n2. Create a tokenizer for the `text-embedding-ada-002` model using the `.encoding_for_model()` method\n3. Count tokens in each document using the `.encode()` method\n4. Calculate the sum of all tokens\n5. Calculate a cost estimate. The `text-embedding-ada-002` model costs `$0.0004` for 1000 tokens","metadata":{},"id":"e87b327b-a0cd-4e4a-8e59-5849317c4f22","cell_type":"markdown"},{"source":"# Import tiktoken\nimport tiktoken\n\n# Create an encoder \nencoder = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n\n# Count tokens in each document\ndoc_tokens = [len(encoder.encode(doc.page_content)) for doc in documents]\n\n# Calculate the sum of all token counts\ntotal_tokens = sum(doc_tokens)\n\n# Calculate a cost estimate\ncost = (total_tokens/1000) * 0.0004\nprint(f\"Total tokens: {total_tokens} - cost: ${cost:.2f}\")","metadata":{"executionCancelledAt":null,"executionTime":1180,"lastExecutedAt":1686670725397,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import tiktoken\nimport tiktoken\n\n# Create an encoder \nencoder = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n\n# Count tokens in each document\ndoc_tokens = [len(encoder.encode(doc.page_content)) for doc in documents]\n\n# Calculate the sum of all token counts\ntotal_tokens = sum(doc_tokens)\n\n# Calculate a cost estimate\ncost = (total_tokens/1000) * 0.0004\nprint(f\"Total tokens: {total_tokens} - cost: ${cost:.2f}\")","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"id":"7a331cba-51cc-4097-b077-ea91222858d9","cell_type":"code","execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":"Total tokens: 1530817 - cost: $0.61\n"}]},{"source":"## Task 4: embed the documents and store embeddings in the vector database\n\nWe're now ready to embed our documents. Since embedding costs money, we'll want to save the embeddings into a database. LangChain can take care of all that using a [Vector Store](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html).\n\nThere are plenty of vector stores to choose from (see the [full list](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)). Today we'll use [Chroma](https://docs.trychroma.com/), but you could be using any other as they have the same interface in LangChain. Once again you'll need to try many of them to see which best fits your use case: some vector stores have specific features (like multimodality or multilingual), so be sure to check them out.\n\nChroma is simple to use and can be persisted to disk. If you do not whish to embed the full set of documents yourself, feel free to skip this step and use the provided folder `chroma-data-langchain-docs`: we've already embedded all documents and persisted it in this folder.\n\n### Instructions\n1. Import `Chroma` from `langchain.vectorstores`\n2. Import `OpenAIEmbeddings` from `langchain.embeddings.openai` \n3. Create the embedding function\n4. Create a database from our documents, using `Chroma.from_documents()`. Pass the documents, embedding function and `persist_directory`.  \n   **Warning: executing this will embed thousands of documents and will cost about $0.6**\n5. Persist the data to disk by calling `.persist()` on the database","metadata":{},"id":"dbd4d3cf-f8ad-4acf-8eae-4a447bac06d4","cell_type":"markdown"},{"source":"# Import chroma\nfrom langchain.vectorstores import Chroma\n\n# Import OpenAIEmbeddings\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Create the mebedding function\nembedding_function = OpenAIEmbeddings()\n\n# Create a database from the documents and embedding function\ndb = Chroma.from_documents(documents=documents, embedding=embedding_function, persist_directory=\"my-embeddings\")\n\n# Persist the data to disk\ndb.persist()","metadata":{"executionCancelledAt":null,"executionTime":52300,"lastExecutedAt":1686670997710,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import chroma\nfrom langchain.vectorstores import Chroma\n\n# Import OpenAIEmbeddings\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Create the mebedding function\nembedding_function = OpenAIEmbeddings()\n\n# Create a database from the documents and embedding function\ndb = Chroma.from_documents(documents=documents, embedding=embedding_function, persist_directory=\"my-embeddings\")\n\n# Persist the data to disk\ndb.persist()"},"id":"dc855f71-5844-4bc4-a6ce-4911e205c362","cell_type":"code","execution_count":32,"outputs":[]},{"source":"## Alternative: use the provided embeddings\n\nWe have already executed the step above to embed all documents and stored the result in the `chroma-data-langchain-docs` folder. Instead of embedding all the documents yourself, you can use these embeddings at no cost.\n\nThe result of this step is the same as the step above, but will not call the OpenAI API and cost nothing.\n\n### Instructions\n1. Import `Chroma` from `langchain.vectorstores`\n2. Import `OpenAIEmbeddings` from `langchain.embeddings.openai`\n3. Create the embedding function\n4. Load the database from the `chroma-data-langchain-docs` directory, and provide the embedding function","metadata":{},"id":"d039e436-3f45-4625-adf3-6ffbd2b00442","cell_type":"markdown"},{"source":"# Import chroma\nfrom langchain.vectorstores import Chroma\n\n# Import OpenAIEmbeddings\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Create the embedding function\nembedding = OpenAIEmbeddings()\n\n# Load the database from existing embeddings\ndb = Chroma(persist_directory=\"chroma-data-langchain-docs\", embedding_function=embedding)","metadata":{"executionCancelledAt":null,"executionTime":1352,"lastExecutedAt":1686646635503,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import chroma\nfrom langchain.vectorstores import Chroma\n\n# Import OpenAIEmbeddings\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Create the embedding function\nembedding = OpenAIEmbeddings()\n\n# Load the database from existing embeddings\ndb = Chroma(persist_directory=\"chroma-data-langchain-docs\", embedding_function=embedding)"},"id":"0ab016d0-a2aa-43d4-8940-8881b53aabfa","cell_type":"code","execution_count":14,"outputs":[]},{"source":"## Step 5: query the vector database\n\nNow that we have a vector database, we can query it. A vector database stores embeddings (vectors) and allow to search through them using K-Nearest Neighbors algorithm (or a variation of it). When we query it the following will happen:\n1. Embed the text query to obtain a vector. It is crucial that this embedding is made using the same embedding technique that was used to embed the documents;\n2. Calculate the distance (or similarity) between the query vector and all other vectors;\n3. Sort results by similarity;\n4. Return the most similar documents.\n\nTo do this with LangChain, we can use the `.similarity_search_with_score()` method of the database.\n\n### Instructions\n1. Call the `similarity_search_with_score` on `db` with the search query as parameter. Store the results in `results`;\n2. Print the results; `results` is a list of tuples like this: `[(doc, score), (doc, score), ...]`\n","metadata":{},"id":"ea396f62-8ea0-49f3-9739-7d677b75a82d","cell_type":"markdown"},{"source":"# Call the `similarity_search_with_score` method on `db`\nresults = db.similarity_search_with_score(\"how do i load data from wikipedia?\")\n\n# Print the results\nfor (doc, score) in results:\n    print('score', score)\n    print(doc.page_content)\n    print('-----------------')","metadata":{"executionCancelledAt":null,"executionTime":442,"lastExecutedAt":1686671097014,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Call the `similarity_search_with_score` method on `db`\nresults = db.similarity_search_with_score(\"how do i load data from wikipedia?\")\n\n# Print the results\nfor (doc, score) in results:\n    print('score', score)\n    print(doc.page_content)\n    print('-----------------')","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"d510e945-335d-4db6-b3aa-75e5753373a9","cell_type":"code","execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":"score 0.2888728082180023\n.ipynb\n.pdf\nWikipedia\n Contents \nInstallation\nExamples\nWikipedia#\nWikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.\nThis notebook shows how to load wiki pages from wikipedia.org into the Document format that we use downstream.\nInstallation#\nFirst, you need to install wikipedia python package.\n#!pip install wikipedia\nExamples#\nWikipediaLoader has these arguments:\nquery: free text which used to find documents in Wikipedia\noptional lang: default=â€enâ€. Use it to search in a specific language part of Wikipedia\noptional load_max_docs: default=100. Use it to limit number of downloaded documents. It takes time to download all 100 documents, so use a small number for experiments. There is a hard limit of 300 for now.\n-----------------\nscore 0.3242723345756531\n.ipynb\n.pdf\nWikipedia\n Contents \nInstallation\nExamples\nRunning retriever\nQuestion Answering on facts\nWikipedia#\nWikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.\nThis notebook shows how to retrieve wiki pages from wikipedia.org into the Document format that is used downstream.\nInstallation#\nFirst, you need to install wikipedia python package.\n#!pip install wikipedia\nWikipediaRetriever has these arguments:\noptional lang: default=â€enâ€. Use it to search in a specific language part of Wikipedia\noptional load_max_docs: default=100. Use it to limit number of downloaded documents. It takes time to download all 100 documents, so use a small number for experiments. There is a hard limit of 300 for now.\n-----------------\nscore 0.3408529460430145\n.md\n.pdf\nWikipedia\n Contents \nInstallation and Setup\nDocument Loader\nRetriever\nWikipedia#\nWikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.\nInstallation and Setup#\npip install wikipedia\nDocument Loader#\nSee a usage example.\nfrom langchain.document_loaders import WikipediaLoader\nRetriever#\nSee a usage example.\nfrom langchain.retrievers import WikipediaRetriever\nprevious\nWhyLabs\nnext\nWolfram Alpha\n Contents\n  \nInstallation and Setup\nDocument Loader\nRetriever\nBy Harrison Chase\n    \n      Â© Copyright 2023, Harrison Chase.\n      \n  Last updated on Jun 06, 2023.\n-----------------\nscore 0.35566750168800354\n.ipynb\n.pdf\nWikipedia\nWikipedia#\nWikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.\nFirst, you need to install wikipedia python package.\n!pip install wikipedia\nfrom langchain.utilities import WikipediaAPIWrapper\nwikipedia = WikipediaAPIWrapper()\nwikipedia.run('HUNTER X HUNTER')\n-----------------\n"}]},{"source":"## Step 6: Create a QA chain\n\nLet's put it all together into a chat-like application. We want the user to ask a question, then search for relevant documents. We'll then create a prompt that includes the documents and the question so GPT can answer it (if possible).\n\nFirst, we'll query the database in a similar manner to previous step. We'll use `.similarity_search()`:\n\n```python\nquestion = \"show an example of adding memory to a chain\"\ncontext_docs = db.similarity_search(question)\n```\n\nNext, we will create a prompt that contains the question and the relevant documents:\n\n> You can think of a PromptTemplate as an fstring in python: values in curly brances are used as placeholder and will be replaced by values we pass when running the chain.\n\n```python\nprompt = PromptTemplate(\n    template=\"\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n<context>\n{context}\n</context>\n\nQuestion: {question}\nHelpful Answer:\"\"\",\n    input_variables=[\"context\", \"question\"]\n)\n```\n\nTo call the LLM with this prompt, we need to create an `LLMChain` and pass it an LLM and the prompt:\n\n```python\nllm = ChatOpenAI(temperature=0)\nqa_chain = LLMChain(llm=llm, prompt=prompt)\n```\n\nWe can now call our chain like so:\n\n```python\nqa_chain({\"context\": \"<the context>\", \"question\": \"<the question>\"})\n```\n\nThis will return a dict with a `text` key containing the LLM response.\n\n### Instructions\n\n1. Import the necessary pieces:  \n   ```python\n   from langchain.prompts import PromptTemplate\n   from langchain.chains.llm import LLMChain\n   from langchain.chat_models import ChatOpenAI\n   ```\n2. Store the question as `question`, query the database and store the result as `context_docs`\n3. Create a prompt with 2 variables: `context` and `question`\n4. Create a chain with an LLM and the prompt\n5. Call the chain and print the result. The LLM output is in the `text` key\n","metadata":{},"id":"6ca2c073-bff4-4d3b-be50-bf541a1d02b2","cell_type":"markdown"},{"source":"# Import \nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chat_models import ChatOpenAI\n\n# Set the question variable\nquestion = \"show an example of adding memory to a chain\"\n\n# Query the database as store the results as `context_docs`\ncontext_docs = db.similarity_search(question)\n\n# Create a prompt with 2 variables: `context` and `question`\nprompt = PromptTemplate(\n    template=\"\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n<context>\n{context}\n</context>\n\nQuestion: {question}\nHelpful Answer, formatted in markdown:\"\"\",\n    input_variables=[\"context\", \"question\"]\n)\n\n# Create an LLM with ChatOpenAI\nllm = ChatOpenAI(temperature=0)\n\n# Create the chain\nqa_chain = LLMChain(llm=llm, prompt=prompt)\n\n# Call the chain\nresult = qa_chain({\n    \"question\": question,\n    \"context\": \"\\n\".join([doc.page_content for doc in context_docs])\n})\n\n# Print the result\nprint(result[\"text\"])\n\n","metadata":{"executionCancelledAt":null,"executionTime":10226,"lastExecutedAt":1686671855410,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import \nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chat_models import ChatOpenAI\n\n# Set the question variable\nquestion = \"show an example of adding memory to a chain\"\n\n# Query the database as store the results as `context_docs`\ncontext_docs = db.similarity_search(question)\n\n# Create a prompt with 2 variables: `context` and `question`\nprompt = PromptTemplate(\n    template=\"\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n<context>\n{context}\n</context>\n\nQuestion: {question}\nHelpful Answer, formatted in markdown:\"\"\",\n    input_variables=[\"context\", \"question\"]\n)\n\n# Create an LLM with ChatOpenAI\nllm = ChatOpenAI(temperature=0)\n\n# Create the chain\nqa_chain = LLMChain(llm=llm, prompt=prompt)\n\n# Call the chain\nresult = qa_chain({\n    \"question\": question,\n    \"context\": \"\\n\".join([doc.page_content for doc in context_docs])\n})\n\n# Print the result\nprint(result[\"text\"])\n\n","outputsMetadata":{"0":{"height":297,"type":"stream"}}},"id":"5e8bd36b-ad02-4641-a188-ce4e957d68f9","cell_type":"code","execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":"```\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\nconversation = ConversationChain(llm=chat, memory=ConversationBufferMemory())\nconversation.run(\"Answer briefly. What are the first 3 colors of a rainbow?\")\n# -> The first three colors of a rainbow are red, orange, and yellow.\nconversation.run(\"And the next 4?\")\n# -> The next four colors of a rainbow are green, blue, indigo, and violet.\n'The next four colors of a rainbow are green, blue, indigo, and violet.'\n```\nThis code shows an example of adding memory to a ConversationChain object using the ConversationBufferMemory class. The memory allows the chain to persist data across multiple calls, making it a stateful object. The example demonstrates how the chain can remember the previous question and provide a response based on that memory.\n"}]},{"source":"from langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\nconversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())\nconversation.run(\"Answer briefly. What are the first 3 colors of a rainbow?\")\n# -> The first three colors of a rainbow are red, orange, and yellow.\nconversation.run(\"And the next 4?\")\n\n","metadata":{"executionCancelledAt":null,"executionTime":2951,"lastExecutedAt":1686671911770,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\nconversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())\nconversation.run(\"Answer briefly. What are the first 3 colors of a rainbow?\")\n# -> The first three colors of a rainbow are red, orange, and yellow.\nconversation.run(\"And the next 4?\")\n\n"},"id":"a160fc6a-cbaa-43bd-b391-1fd77ff038a2","cell_type":"code","execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":"'The next four colors of a rainbow are green, blue, indigo, and violet.'"},"metadata":{},"execution_count":40}]},{"source":"## To go further\n\nOur little chat app is working, but can be improved. Consider the following improvements:\n- **Clean up the documents**.  \n  Each document currently starts with useless text, and ends with a copyright notice. These texts do not provide value in our case and should be removed before embedding. Try to make an additional step after loading the `raw_documents`. In this step, iterate over all documents and find a way to remove unnecessary text.\n- **Add streaming to the LLM**.  \n  Instead of waiting for the full response, you can get a better experience by streaming the LLM response (much like the ChatGPT web interface). Look at the `playground.ipynb` notebook for an example, or check the [docs](https://python.langchain.com/en/latest/modules/models/chat/examples/streaming.html).\n- **Return sources to the user**.  \n  Getting a response is nice, but linking to the relevant docs is even better. There are a few techniques to return the source documents to the user. The simplest is to print the metadata of all the `context_docs` returned by the semantic search. In this example, you could use `doc.metadata[\"source\"]` to create a link to the langchain docs. A more advanced technique can be found [here](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa_with_sources.html).\n- **Add memory support to the chain**.  \n  You can pass a `memory` argument to the `LLMChain`. Since we have multiple input variables, make sure to specify which one the memory should use: `ConversationBufferMemory(input_key=\"question\")`. You will also need to rerite the question before searching for documents, as the full context will not always be contained in the user input. You can find a working example in `going-further/adding-memory.ipynb`\n\n<br />\n\n### Useful links\n- Learn more about embeddings: https://txt.cohere.com/sentence-word-embeddings/\n- Learn more about vector databases: https://www.pinecone.io/learn/vector-database/\n","metadata":{},"id":"f633b2cc-dbdf-40f2-9561-a1227d8b8a7d","cell_type":"markdown"},{"source":"## LangChain caveats\n\nWhile LangChain is a great tool to discover concepts and techniques, it falls short to help you deliver a production-ready application:\n- Documentation is lacking. The documentation is not very complete and mostly made of examples, it lacks explanations and proper descriptions. To fully discover how a given module is working, you will need to peek in the code ([here on GitHub](https://github.com/hwchase17/langchain))\n- Too many abstractions. LangChain brings a lot of classes, some would argue too many. A `PromptTemplate` for example is pretty much the equivalent of a simple python f-string.\n- Very opinionated. As soon as you want to customize a chain (or agent), you will find that it's not always the smost simple thing. Many end-up re-implementing LangChain classes or functions to suit their needs better.\n\nLangChain is a great starting point and will let you write compelling demos very quickly. Consider these as prototypes, and not something that can be brought to production.","metadata":{},"id":"dff39e9a-798f-42d9-8f71-3aaecb435a84","cell_type":"markdown"}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}